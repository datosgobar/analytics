{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T00:05:45.394905Z",
     "start_time": "2019-08-02T00:05:44.545604Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# para importar módulos en '../../tools'\n",
    "module_path = os.path.abspath(os.path.join('../../tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import locale\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from pylab import *\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "from urllib.parse import parse_qs\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import maxabs_scale, minmax_scale, MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "# import squarify\n",
    "\n",
    "import json\n",
    "\n",
    "import analytics_tools\n",
    "import requests\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "plt.rcParams['image.cmap'] = 'Accent'\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('precision',2)\n",
    "pd.set_option('display.float_format', lambda x: locale.format_string('%d', x, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T00:05:47.232466Z",
     "start_time": "2019-08-02T00:05:46.144398Z"
    }
   },
   "outputs": [],
   "source": [
    "# actualizo los analytics de ambas apis\n",
    "analytics_tools.update_analytics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T13:55:50.751705Z",
     "start_time": "2019-06-24T13:55:50.737222Z"
    }
   },
   "outputs": [],
   "source": [
    "# formarters\n",
    "f_ar = lambda x: locale.format_string('%.2f', x, 1)\n",
    "d_ar = lambda x: locale.format_string('%d', x, 1)\n",
    "\n",
    "tz_arg = lambda x: pd.to_datetime(x).tz_localize('UTC').tz_convert(tz='America/Argentina/Buenos_Aires')\n",
    "\n",
    "TABLE_COUNTER = 0\n",
    "TABLE_TEMPLATE = \"\"\"\n",
    "<center><strong><small>{title}</small></strong></center>\n",
    "<center>{table}</center>\n",
    "<center><strong><small>Tabla {table_number}</small></strong></center>\n",
    "\"\"\"\n",
    "    \n",
    "def table_counter():\n",
    "    global TABLE_COUNTER\n",
    "    TABLE_COUNTER += 1\n",
    "    return TABLE_COUNTER\n",
    "\n",
    "def add_style_to_df(df, subset, color='black', font_weight=None):\n",
    "    render = df.style.set_properties(\n",
    "        subset= subset, \n",
    "        **{'font-weight': font_weight, 'color':color}).render().replace('\\n','')\n",
    "    return render\n",
    "\n",
    "def add_title(df_html, title):\n",
    "    str_table = TABLE_TEMPLATE.format(title=title, table=df_html, table_number=table_counter())\n",
    "    return str_table.replace('\\n','')\n",
    "\n",
    "def put_df_on_report(df, title, subset=None, color='black', font_weight='bold'):\n",
    "    if subset:\n",
    "        df_html = add_style_to_df(df, subset=subset,color=color, font_weight=font_weight)\n",
    "    else:\n",
    "        df_html = df.to_html().replace('\\n','')\n",
    "    return add_title(df_html, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T13:55:50.784415Z",
     "start_time": "2019-06-24T13:55:50.753935Z"
    }
   },
   "outputs": [],
   "source": [
    "midpoint = lambda x: (np.max(x)+ np.min(x))/2\n",
    "log_midpoint = lambda x: np.power(10,((np.log10(np.max(x))+np.log10(np.min(x)))/2))\n",
    "\n",
    "def read_files_to_df(directory):\n",
    "    \"\"\"Lee CSVs de misma estructura en un directorio a un solo DataFrame.\"\"\"\n",
    "\n",
    "    file_pattern = os.path.join(directory, \"*.csv\")\n",
    "    dfs = [pd.read_csv(file, encoding=\"utf8\", parse_dates=True)\n",
    "           for file in glob.glob(file_pattern)]\n",
    "        \n",
    "    return pd.concat(dfs, axis=0)\n",
    "\n",
    "def parse_ids(qs):\n",
    "    if pd.notna(qs):\n",
    "        if 'ids' in qs:\n",
    "            params = parse_qs(qs)\n",
    "            ids_values_str = params.get('ids',[''])[0]\n",
    "        else:\n",
    "            ids_values_str = qs\n",
    "\n",
    "        return [value.split(':')[0] for value in ids_values_str.split(',')]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def iter_series_ids(df):\n",
    "    for analytic_id, id_list in zip(df.id, df.ids):\n",
    "            if isinstance(id_list, list):\n",
    "                for serie_id in id_list:\n",
    "                    yield (analytic_id, serie_id)\n",
    "    \n",
    "def unfold_series_id(df):\n",
    "    list_ids = []\n",
    "    for analytic_id, serie_id in iter_series_ids(df):\n",
    "        list_ids.append([analytic_id, serie_id])\n",
    "        \n",
    "    df_ids = pd.DataFrame(list_ids, columns=['analytic_id','serie_id'])\n",
    "    df_ids.dropna(inplace=True)\n",
    "    \n",
    "    df_ids['analytic_id'] = df_ids['analytic_id'].astype(object)\n",
    "    \n",
    "    df_extended = df_ids.merge(df, left_on='analytic_id', right_on='id')\n",
    "    df_extended = df_extended.drop(columns=['analytic_id'])\n",
    "    \n",
    "    return df_extended\n",
    "\n",
    "def replace_and_drop_column(df, old_column, new_column):\n",
    "    df[new_column] = df[old_column]\n",
    "    new_df = df.drop(old_column, axis=1)\n",
    "    return new_df\n",
    "\n",
    "def add_totals(df):\n",
    "    df_to_append = pd.DataFrame(df.sum(numeric_only=True)).T\n",
    "    \n",
    "    df_appended = df.append(df_to_append)\n",
    "    \n",
    "    as_list = df_appended.index.tolist()\n",
    "    idx = as_list.index(0)\n",
    "    as_list[idx] = 'Total'\n",
    "    df_appended.index = as_list\n",
    "    \n",
    "    return df_appended\n",
    "\n",
    "def add_linear_regression(df, target_col, index_as_feature=True, feature_cols=''):\n",
    "    if index_as_feature:\n",
    "        X = df.index.values.reshape(-1, 1)\n",
    "    else:\n",
    "        n_r, n_c = df[feature_cols].shape\n",
    "        X = df[feature_cols].values.reshape(-1, n_c)\n",
    "        \n",
    "    y = df[target_col].values\n",
    "    \n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X, y)\n",
    "    \n",
    "#     df['linear_regression'] = \n",
    "    return reg.predict(X)\n",
    "\n",
    "#\n",
    "def interpolate_by_days(df, column, date_col='' ,days=1, floor=0):\n",
    "    df['day_factor'] = pd.DatetimeIndex(df[date_col]).dayofyear % days\n",
    "    day_factor_unique = df.day_factor.unique()\n",
    "    list_df = []\n",
    "\n",
    "    call_col_inter = column + '_inter'\n",
    "    \n",
    "    for day_factor in day_factor_unique:\n",
    "#         for call_col in columns:\n",
    "        \n",
    "        filter_by_df = df.day_factor == day_factor\n",
    "        df_f = df[filter_by_df]\n",
    "        df_f[call_col_inter] = df_f[column].apply(lambda x: np.nan if x < floor else x)\n",
    "        df_f.loc[:,call_col_inter] = df_f[call_col_inter].interpolate(method='linear', limit_direction='both',inplace=False)\n",
    "        list_df.append(df_f)\n",
    "        del df_f\n",
    "\n",
    "    df_inter = pd.concat(list_df, sort=True)\n",
    "    \n",
    "    return df_inter[call_col_inter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T13:55:50.794161Z",
     "start_time": "2019-06-24T13:55:50.787146Z"
    }
   },
   "outputs": [],
   "source": [
    "# para retención de usr\n",
    "def get_week(d):\n",
    "    start = d - pd.Timedelta(days=d.weekday())\n",
    "    end = start + pd.Timedelta(days=6)\n",
    "    return end\n",
    "\n",
    "def cohort_period(df):\n",
    "    df['cohort_period'] = np.arange(len(df)) + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T13:55:50.828963Z",
     "start_time": "2019-06-24T13:55:50.797570Z"
    }
   },
   "outputs": [],
   "source": [
    "# para clusterizar\n",
    "# recibe un unfolded\n",
    "def get_nunique_series_avg(df, frequency='W'):\n",
    "    \"\"\"Devuelve el promedio de cantidad de series únicas por la frencuencia indicada \"\"\"\n",
    "    df_list = []\n",
    "    \n",
    "    for frequency in 'DWM':\n",
    "        if frequency is 'D':\n",
    "            col = 'date'\n",
    "            frec_col = 'series_unicas_diarias'\n",
    "        elif frequency is 'W':\n",
    "            col = 'weekofyear'\n",
    "            frec_col = 'series_unicas_semanales'\n",
    "        elif frequency is 'M':\n",
    "            col = 'month'\n",
    "            frec_col = 'series_unicas_mensuales'\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        df_avg_series = df.groupby(['ip_address',col]).nunique()['serie_id'].reset_index()\n",
    "        \n",
    "#         n_series_unicas = df_avg_series.serie_id.count()\n",
    "        \n",
    "        df_avg_series = df_avg_series[['ip_address', 'serie_id']].groupby('ip_address').mean()\n",
    "        df_avg_series.rename({'serie_id':frec_col},axis=1, inplace=True) \n",
    "                \n",
    "        df_list.append(df_avg_series)\n",
    "\n",
    "    df_series_avg = pd.concat(df_list, axis=1).reset_index()\n",
    "    \n",
    "    ass = df[['month','ip_address', 'serie_id','indice_tiempo_frecuencia']].drop_duplicates()\n",
    "    id_unicos_por_ip_mes = ass.groupby(['ip_address','month']).nunique()['serie_id'].reset_index()\n",
    "    \n",
    "    serie_diaria = ass.indice_tiempo_frecuencia.str.contains('R/P1D')\n",
    "    id_diarios_unicos_por_ip_mes = ass[serie_diaria].groupby(['ip_address','month']).nunique()['serie_id'].reset_index()\n",
    "    \n",
    "#     ### ip - month - su - sud\n",
    "#     ass = df[['date_short','ip_address', 'serie_id','indice_tiempo_frecuencia']].drop_duplicates()\n",
    "#     id_unicos_por_ip_mes = ass.groupby(['ip_address','date_short']).nunique()['serie_id'].reset_index()\n",
    "#     id_unicos_por_ip_mes.rename({'serie_id':'series_unicas'}, axis=1, inplace=True)\n",
    "#     serie_diaria = ass.indice_tiempo_frecuencia.str.contains('R/P1D')\n",
    "#     id_diarios_unicos_por_ip_mes = ass[serie_diaria].groupby(['ip_address','date_short']).nunique()['serie_id'].reset_index()\n",
    "#     id_diarios_unicos_por_ip_mes.rename({'serie_id':'series_diarias_unicas'}, axis=1, inplace=True)\n",
    "\n",
    "#     #n_id_diarios_unicos = id_diarios_unicos_por_ip_mes.groupby('ip_address').mean().reset_index()\n",
    "#     asss = id_unicos_por_ip_mes.merge(id_diarios_unicos_por_ip_mes, on=['ip_address','date_short'], how='left').fillna(0)\n",
    "\n",
    "#     ####\n",
    "#     assss = asss[['ip_address','series_unicas','series_diarias_unicas']].groupby('ip_address').mean()\n",
    "\n",
    "    ### ip - su - sud\n",
    "    ass = df[['ip_address', 'serie_id','indice_tiempo_frecuencia']].drop_duplicates()\n",
    "    id_unicos_por_ip_mes = ass.groupby('ip_address').nunique()['serie_id'].reset_index()\n",
    "    id_unicos_por_ip_mes.rename({'serie_id':'series_unicas'}, axis=1, inplace=True)\n",
    "    serie_diaria = ass.indice_tiempo_frecuencia.str.contains('R/P1D')\n",
    "    id_diarios_unicos_por_ip_mes = ass[serie_diaria].groupby('ip_address').nunique()['serie_id'].reset_index()\n",
    "    id_diarios_unicos_por_ip_mes.rename({'serie_id':'series_diarias_unicas'}, axis=1, inplace=True)\n",
    "\n",
    "    #n_id_diarios_unicos = id_diarios_unicos_por_ip_mes.groupby('ip_address').mean().reset_index()\n",
    "    asss = id_unicos_por_ip_mes.merge(id_diarios_unicos_por_ip_mes, on='ip_address', how='left').fillna(0)\n",
    "\n",
    "    ####\n",
    "    assss = asss\n",
    "\n",
    "    df_series_avg = df_series_avg.merge(assss, on='ip_address').fillna(0)\n",
    "\n",
    "    return df_series_avg\n",
    "\n",
    "\n",
    "# recibe un unfolded!\n",
    "def get_ips_calls_max(df, frequency='D'):\n",
    "    \"\"\"Devuelve el promedio diario de llamadas por IP \"\"\"\n",
    "    if frequency is 'W':\n",
    "        df_ncalls = df.groupby(['ip_address','weekofyear']).nunique()['id'].reset_index()\n",
    "    elif frequency is 'D':\n",
    "        df_ncalls = df.groupby(['ip_address','date']).nunique()['id'].reset_index()\n",
    "    else:   \n",
    "        return\n",
    "    \n",
    "    df_ncalls = df_ncalls.groupby(['ip_address']).max().reset_index()\n",
    "    df_ncalls.rename({'id':'max_calls'},axis=1, inplace=True)\n",
    "    df_ncalls.sort_values('max_calls', ascending=False, inplace=True)\n",
    "\n",
    "    return df_ncalls\n",
    "\n",
    "# recibe un unfolded!\n",
    "def get_ips_calls_count(df, frequency='D', scale='linear'):\n",
    "    \"\"\"Devuelve el promedio diario de llamadas por IP \"\"\"\n",
    "    if frequency is 'W':\n",
    "        df_ncalls = df.groupby(['ip_address','weekofyear']).nunique()['id'].reset_index()\n",
    "    elif frequency is 'D':\n",
    "        df_ncalls = df.groupby(['ip_address','date']).nunique()['id'].reset_index()\n",
    "    else:   \n",
    "        return\n",
    "    \n",
    "    df_ncalls = df_ncalls.groupby(['ip_address']).mean().reset_index()\n",
    "    df_ncalls.rename({'id':'actividad'},axis=1, inplace=True)\n",
    "    df_ncalls.sort_values('actividad', ascending=False, inplace=True)\n",
    "\n",
    "    if scale is 'log':\n",
    "        df_ncalls['actividad'] = df_ncalls.actividad.apply(np.log10)\n",
    "    \n",
    "    return df_ncalls\n",
    "\n",
    "# recibe un unfolded!\n",
    "def get_ips_persistency(df, frequency='W', scale='linear'):\n",
    "    \"\"\"Devuelve la proporción de semanas con actividad para cada IP\n",
    "    Args:\n",
    "        frequency: str. W o D\n",
    "    \"\"\"\n",
    "    if frequency is 'W':\n",
    "#         group_cols = ['year','month','ip_address']\n",
    "#         data_cols = group_cols + ['weekofyear']\n",
    "#         n_frec = df[['year','month','weekofyear']].groupby(['year','month']).nunique()[['weekofyear']].sum().values[0]\n",
    "#         df_persistency = df[data_cols].groupby(group_cols).nunique()[['weekofyear']]\n",
    "\n",
    "        group_cols = ['ip_address']\n",
    "        data_cols = group_cols + ['weekdate']\n",
    "        n_frec = df['weekdate'].nunique()\n",
    "        df_persistency = df[data_cols].groupby(group_cols).nunique()[['weekdate']]\n",
    "        \n",
    "#         print(n_frec)\n",
    "\n",
    "    elif frequency is 'D':\n",
    "        group_cols = ['ip_address']\n",
    "        data_cols = group_cols + ['date']\n",
    "\n",
    "        n_frec = df.date.nunique()\n",
    "\n",
    "        df_persistency = df[data_cols].groupby(group_cols).nunique()[['date']]\n",
    "    else:   \n",
    "        return\n",
    "    \n",
    "    df_persistency = df_persistency.unstack(level=0).unstack(level=0)\n",
    "    df_persistency = df_persistency.fillna(0).sum(1).reset_index()\n",
    "    df_persistency.rename({0:'persistencia'},axis=1, inplace=True)\n",
    "    df_persistency.sort_values('persistencia', ascending=False, inplace=True)\n",
    "    df_persistency['persistencia'] = df_persistency.persistencia.divide(n_frec/100)\n",
    "    \n",
    "    if scale is 'log':\n",
    "        df_persistency['persistencia'] = df_persistency.persistencia.apply(np.log10)\n",
    "\n",
    "    return df_persistency\n",
    "\n",
    "# recibe un unfolded!\n",
    "def get_ip_features(df, frequencies={'persistencia':'W','actividad':'D'}, scales={'persistencia':'linear','actividad':'log'}):\n",
    "    \n",
    "    df_ncalls = get_ips_calls_count(df, frequency=frequencies['actividad'], scale=scales['actividad'])\n",
    "    df_persistency = get_ips_persistency(df, frequency=frequencies['persistencia'], scale=scales['persistencia'])\n",
    "#     df_maxcalls = get_ips_calls_max(df)\n",
    "\n",
    "    df_ip_features = df_ncalls.merge(df_persistency, how='inner', on='ip_address')\n",
    "#     df_ip_features = df_ip_features.merge(df_maxcalls, how='inner', on='ip_address')\n",
    "    \n",
    "    return df_ip_features\n",
    "\n",
    "# \n",
    "def cluster_ips(df, n_kmeans=3, frequencies={'persistencia':'W','actividad':'D'}, scales={'persistencia':'linear','actividad':'log'},labels=None):\n",
    "    df_ip_features = get_ip_features(df, frequencies, scales)\n",
    "\n",
    "    X = df_ip_features[['actividad','persistencia']].values.reshape(-1, 2)\n",
    "    X_minmax = MinMaxScaler().fit_transform(X)\n",
    "#     X_minmax = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    clusters_id = KMeans(n_kmeans, random_state=0).fit_predict(X_minmax)\n",
    "\n",
    "    ips_clusterizados = df_ip_features.merge(pd.DataFrame(clusters_id), how='inner', left_index=True, right_index=True)\n",
    "    ips_clusterizados.rename({0:'cluster_id'}, axis=1, inplace=True)\n",
    "    ips_clusterizados['cluster_name'] = ips_clusterizados.cluster_id.apply(lambda x: labels[x])\n",
    "    return ips_clusterizados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T13:55:50.878085Z",
     "start_time": "2019-06-24T13:55:50.831251Z"
    }
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def make_cluster_plots(df_cluster, clusters, exclude_clusters_id=None):\n",
    "    \n",
    "    G = gridspec.GridSpec(4, 3)\n",
    "\n",
    "    plt.figure(figsize=[15,20])\n",
    "    \n",
    "    axes_1 = subplot(G[0:2, :])\n",
    "    plt.scatter(df_cluster['actividad'], df_cluster['persistencia'], c=df_cluster['color'], s=50,alpha=.7);\n",
    "    plt.title('Segmentación de usuarios, período completo')\n",
    "    plt.xlabel('actividad')\n",
    "    plt.ylabel('persistencia');\n",
    "\n",
    "    axes_3 = subplot(G[2:,:-1])\n",
    "    act_std = df_cluster.actividad.std()\n",
    "    pers_std = df_cluster.persistencia.std()\n",
    "    act_mean = df_cluster.actividad.mean()\n",
    "    pers_mean = df_cluster.persistencia.mean()\n",
    "\n",
    "    df_cluster['actividad_norm'] = minmax_scale(df_cluster.actividad, feature_range=(act_mean-2*act_std,act_mean+2*act_std))\n",
    "    df_cluster['persistencia_norm'] = minmax_scale(df_cluster.persistencia, feature_range=(pers_mean-2*pers_std,pers_mean+2*pers_std))\n",
    "    \n",
    "    df_cluster_profile = df_cluster[~df_cluster.cluster_id.isin(exclude_clusters_id)].groupby(['cluster_id','cluster_name','color']).agg({'ip_address': pd.Series.nunique,\n",
    "                                            'actividad_norm': pd.Series.mean,\n",
    "                                            'persistencia_norm': pd.Series.mean}).reset_index()\n",
    "\n",
    "    df_cluster_profile = df_cluster_profile.sort_values('ip_address', ascending=False)\n",
    "\n",
    "    with sns.axes_style('white'):\n",
    "    #     fig, ax = plt.subplots(figsize=(15,10))\n",
    "        axes_3.spines['bottom'].set_position('center')\n",
    "        axes_3.spines['left'].set_position('center')\n",
    "        axes_3.spines['bottom'].set_color('grey')\n",
    "        axes_3.spines['left'].set_color('grey')\n",
    "        axes_3.spines['right'].set_color('none')\n",
    "        axes_3.spines['top'].set_color('none')\n",
    "        axes_3.xaxis.set_label_coords(.95, 0.48)\n",
    "        axes_3.yaxis.set_label_coords(0.53, 0.08)\n",
    "        axes_3.set_alpha(.2)\n",
    "\n",
    "        a = np.abs(df_cluster_profile.actividad_norm)\n",
    "        p = np.abs(df_cluster_profile.persistencia_norm)\n",
    "        u = df_cluster_profile.ip_address\n",
    "        cluster_s = u * a\n",
    "\n",
    "        df_cluster_profile.plot.scatter('actividad_norm','persistencia_norm', \n",
    "                                    c=df_cluster_profile.color.values,\n",
    "#                                     c=df_cluster_profile.index.values,\n",
    "                                    s=cluster_s, \n",
    "                                    cmap='Accent', \n",
    "                                    colorbar=False,\n",
    "                                    alpha=.55,\n",
    "                                    legend=True,\n",
    "                                    ax=axes_3);\n",
    "\n",
    "    for i, row in df_cluster_profile.iterrows():\n",
    "        txt = row.cluster_name\n",
    "        x = row.actividad_norm\n",
    "        y = row.persistencia_norm\n",
    "        axes_3.text(x, y, txt, horizontalalignment='center',verticalalignment='bottom')\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.xlabel('actividad');\n",
    "    plt.ylabel('persistencia');\n",
    "    plt.title('Variables normalizadas.');\n",
    "\n",
    "    df_cluster_agg = df_cluster.groupby(['cluster_id','cluster_name','color']).agg({'ip_address': pd.Series.nunique,\n",
    "                                                           'actividad': pd.Series.sum}).reset_index()\n",
    "    df_cluster_agg = df_cluster_agg.sort_values('cluster_id')\n",
    "\n",
    "    col = 'ip_address'\n",
    "    title = 'Usuarios'\n",
    "    axes = subplot(G[2,-1])\n",
    "\n",
    "\n",
    "    patches = df_cluster_agg[col].plot(kind='pie',autopct='%.1f',labels=None, fontsize=0, colors=df_cluster_agg.color.values, ax=axes)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('');\n",
    "    labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(df_cluster_agg.cluster_name, df_cluster_agg[col].divide(df_cluster_agg[col].sum())*100)]\n",
    "    plt.legend(labels=labels, loc='best')\n",
    "\n",
    "    col = 'actividad'\n",
    "    title = 'Llamadas'\n",
    "    axes = subplot(G[3,-1])\n",
    "\n",
    "    patches = df_cluster_agg[col].plot(kind='pie',autopct='%.1f',labels=None, fontsize=0, colors=df_cluster_agg.color.values, ax=axes)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('');\n",
    "    labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(df_cluster_agg.cluster_name, df_cluster_agg[col].divide(df_cluster_agg[col].sum())*100)]\n",
    "    plt.legend(labels=labels, loc='best')\n",
    "        \n",
    "    plt.savefig('../graphs/graphs-{}-clusters.png'.format(clusters), dpi=64)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T13:55:50.900955Z",
     "start_time": "2019-06-24T13:55:50.880453Z"
    }
   },
   "outputs": [],
   "source": [
    "# esto necesita un superrefactor\n",
    "\n",
    "def pie_plot_w_legend(pd_df=pd.DataFrame(),\n",
    "                      title='',\n",
    "                      colors=None,\n",
    "                      colors_in_df=True,\n",
    "                      ax=None):\n",
    "    \n",
    "#     colors = pd_df.iloc[:,1] if colors is None else colors\n",
    "    txts = pd_df.index\n",
    "    if colors is None and colors_in_df is False:\n",
    "#         if colors_in_df is True:\n",
    "        patches = pd_df.iloc[:,0].plot(kind='pie',autopct='%.2f',labels=['' for _ in pd_df.iloc[:,0]], fontsize=0, ax=ax, figsize=[15,15], startangle=45)\n",
    "    if colors_in_df is True:\n",
    "        colors = pd_df.iloc[:,1]\n",
    "        patches = pd_df.iloc[:,0].plot(kind='pie',autopct='%.2f',labels=['' for _ in pd_df.iloc[:,0]], fontsize=0, ax=ax, figsize=[15,15], colors=colors, startangle=45)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('');\n",
    "    labels = ['{1:0.2f} % - {0}'.format(i,j) for i,j in zip(txts, pd_df.iloc[:,0].divide(pd_df.iloc[:,0].sum())*100)]\n",
    "    plt.legend(labels=labels, loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Uso de series en API Series de Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T13:55:50.913664Z",
     "start_time": "2019-06-24T13:55:50.906289Z"
    }
   },
   "outputs": [],
   "source": [
    "## Datos analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T18:03:51.766950Z",
     "start_time": "2019-06-24T18:03:39.133601Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abenassi/anaconda/envs/analytics/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Cargo csv's\n",
    "# df_analytics = read_files_to_df('/home/melik/sdt-analytics-download/')\n",
    "df_analytics = read_files_to_df(analytics_tools.DIR_DATA_GR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T18:03:54.379558Z",
     "start_time": "2019-06-24T18:03:51.769283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filtro analytics por valores de interés\n",
    "uri_georef = df_analytics.uri.str.contains('/georef/api/')\n",
    "status_ok = df_analytics.status_code == 200\n",
    "status_nan = df_analytics.status_code.isnull()\n",
    "\n",
    "# df_analytics = df_analytics[(uri_serie) & (status_ok | status_nan)]\n",
    "df_analytics = df_analytics[uri_georef & ~status_nan & status_ok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T18:03:54.782142Z",
     "start_time": "2019-06-24T18:03:54.381841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filtro analytics. Saco ip's de test, saco períodos sin datos\n",
    "\n",
    "# ips asociados al pico del 2018-12-21\n",
    "test_ips = ['190.16.55.43','190.210.119.109', '190.246.123.181']\n",
    "# ips asociados al pico del 2018-02-15\n",
    "test_ips.extend(['195.162.12.14','190.18.52.25'])\n",
    "\n",
    "exclude_test_ips = ~df_analytics.ip_address.isin(test_ips)\n",
    "df_analytics = df_analytics[exclude_test_ips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T18:08:00.204036Z",
     "start_time": "2019-06-24T18:03:54.784208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cambio el tipo de dato de la columna 'timestamp' a datetime\n",
    "df_analytics['start_time_utc'] = pd.to_datetime(df_analytics['start_time'])\n",
    "\n",
    "df_analytics['start_time'] = pd.DatetimeIndex(df_analytics['start_time']).tz_convert(tz='America/Argentina/Buenos_Aires')\n",
    "df_analytics[\"date\"] = pd.DatetimeIndex(df_analytics.start_time).normalize()\n",
    "df_analytics[\"hour\"] = pd.DatetimeIndex(df_analytics.start_time).hour\n",
    "df_analytics[\"month\"] = pd.DatetimeIndex(df_analytics.start_time).month\n",
    "df_analytics[\"year\"] = pd.DatetimeIndex(df_analytics.start_time).year\n",
    "df_analytics['weekday'] = pd.DatetimeIndex(df_analytics.start_time).weekday\n",
    "df_analytics['weekofyear'] = pd.DatetimeIndex(df_analytics.start_time).weekofyear\n",
    "df_analytics['weekdate'] = df_analytics[\"date\"].apply(get_week)\n",
    "df_analytics['weekdate_short'] = df_analytics.weekdate.dt.strftime(\"%Y-%m-%d\")\n",
    "df_analytics['date_short'] = pd.DatetimeIndex(df_analytics.date).strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T18:08:07.873485Z",
     "start_time": "2019-06-24T18:08:01.892931Z"
    }
   },
   "outputs": [],
   "source": [
    "# agrego clasificación de qs's\n",
    "qs_contains_name = df_analytics.querystring.str.contains('nombre=').astype(bool)\n",
    "qs_contains_dir = df_analytics.querystring.str.contains('direccion=').astype(bool)\n",
    "uri_is_ubicacion = df_analytics.uri.str.contains('ubicacion').astype(bool)\n",
    "\n",
    "query_is_norm = (qs_contains_name & (~qs_contains_dir) & (~uri_is_ubicacion))\n",
    "query_is_norm_dir = (qs_contains_dir & (~qs_contains_name) & (~uri_is_ubicacion))\n",
    "query_is_ref = ~(query_is_norm | query_is_norm_dir | uri_is_ubicacion)\n",
    "\n",
    "df_analytics.loc[uri_is_ubicacion, 'query_tipo'] = 'enriquecimiento'\n",
    "df_analytics.loc[query_is_norm, 'query_tipo'] = 'normalizacion_nombres'\n",
    "df_analytics.loc[query_is_norm_dir, 'query_tipo'] = 'normalizacion_direcciones'\n",
    "df_analytics.loc[query_is_ref, 'query_tipo'] = 'referenciación'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T18:08:01.891111Z",
     "start_time": "2019-06-24T18:08:00.206527Z"
    }
   },
   "outputs": [],
   "source": [
    "# separo columna endpoint\n",
    "df_analytics['endpoint'] = df_analytics.uri.apply(lambda x: str(x).strip('/').split('/')[-1].split('.')[0])\n",
    "\n",
    "# reemplazo los methods que no son GET o POST por OTROS\n",
    "# other_mr = ~df_analytics.request_method.isin(['GET','POST','HEAD'])\n",
    "# df_analytics.loc[other_mr,'request_method'] = 'OTROS'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T13:59:38.440885Z",
     "start_time": "2019-06-24T13:59:38.079238Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_analytics = df_analytics.merge(pd.get_dummies(df_analytics.query_tipo), right_index=True, left_index=True)\n",
    "\n",
    "df_evolucion_by_tipo = df_analytics[['id','date_short','query_tipo']].groupby(['date_short','query_tipo']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T14:42:06.163773Z",
     "start_time": "2019-06-24T14:42:04.039785Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ips_by_tipo = df_analytics.groupby(['date_short','query_tipo'])['ip_address'].nunique().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T14:00:48.681186Z",
     "start_time": "2019-06-24T14:00:48.318585Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hits = df_analytics.groupby(['request_method','endpoint','query_tipo'])['id'].count()\n",
    "\n",
    "df_hits = df_hits.to_frame().reset_index().rename({'id':'hits'}, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T13:59:38.460819Z",
     "start_time": "2019-06-24T13:55:50.785Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_osde_by_month = df_log[df_log.http_referer.str.contains('osde')].time_local.apply(lambda x: str(x)[:7]).value_counts().to_frame().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T18:09:19.998975Z",
     "start_time": "2019-06-24T18:09:19.775498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan     296810\n",
       "HEAD    45414 \n",
       "Name: request_method, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analytics[~df_analytics.request_method.isin(['GET','POST'])].request_method.fillna('nan').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T18:42:47.683896Z",
     "start_time": "2019-06-24T18:42:47.510257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2018-07</td>\n",
       "      <td>2852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-08</td>\n",
       "      <td>41958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-09</td>\n",
       "      <td>108909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-10</td>\n",
       "      <td>125990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-11</td>\n",
       "      <td>17101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_short\n",
       "2018-07  2852      \n",
       "2018-08  41958     \n",
       "2018-09  108909    \n",
       "2018-10  125990    \n",
       "2018-11  17101     "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analytics[~df_analytics.request_method.isin(['GET','POST','HEAD'])].date_short.value_counts().to_frame().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
